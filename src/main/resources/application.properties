spring.application.name=OfflineAI.ollama
#Server configuration
server.port=3001
server.error.include-message=always
spring.profiles.active=dev
#Ollama configuration
ollama.endpoint=http://localhost:11434/api/generate
ollama.model=deepseek-r1:1.5b
ollama.timeout.connect =40000
ollama.timeout.read =80000